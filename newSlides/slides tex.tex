\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{multicol}
\usetheme{Berlin}
\usecolortheme{beaver}
\usepackage{color}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\begin{document}
\title{The Lasso Method of Parameter Selection}
\author{C. Kinstley, A. Nabar, T. Williams, C. Vollmer }
\frame{\titlepage}

\section[Outline]{}
\frame{
\begin{multicols}{2}
 \tableofcontents[%pausesections
 ]
 
 \end{multicols}
}

\section{Improving OLS}

\frame{
\begin{multicols}{2}
 \tableofcontents[currentsection, hideothersubsections]
\end{multicols}
}

\frame
{
\frametitle{Improving OLS}
	\begin{itemize}
	\item Recall the ordinary least squares procedure (OLS) estimates unknown coefficients in a linear regression model by minimizing the {\color{red}squared difference} between the {\color{purple}predicted} and {\color{blue}actual} responses. 
	
	\item We would like to fit $n$ data points to a model of the form {\color{purple}$$y_1= \beta_0 + \beta_1 x_1 + \beta_2 x_2+ \dots +\beta_{p-1} x_{p-1}$$}
	
	\item Let $X$ denote the $n \times p$ design matrix where the $x_{ij}$th entry is the $i$th point of sample data corresponding to the $j$th dependent variable and $Y$ the response vector
		

	\end{itemize}
}



\frame
{
\begin{itemize}

	\item  So long as $n \geq p$, OLS gives us a best-fit hyper-plane as follows:
    
    \begin{itemize}
    \item The {\color{red}difference}, or the error, between the actual and 
    predicted response for each data point $i \in 1 \dots n$ is
    
    \begin{equation*}
     {\color{red} \epsilon_{i}} =  {\color{blue} Y_{i}} - {\color{purple} X\beta}
     \end{equation*}
    
    \item We minimize the sum of the squared differences, i.e minimize the function
    $$ E({\color{purple}\hat{\beta} }) = \sum_{i=1}^n {\color{red}{\epsilon_i^2}} = \norm{{\color{blue}Y}^2_2- {\color{purple}X\beta}}_2^2$$
    
    \item It can be shown that the minimizers are
    $\hat{\beta} = (X^TX)^{-1}X^TY$ but these solutions are usually approximated
    \end{itemize}
    
\end{itemize}
}

\subsection{What can be improved?}
\frame
{

  \frametitle{Problems with OLS}
  
  Two common problems with OLS estimates are as follows:
  \begin{itemize}
  \item Imprecision
  \item The expected value of the error term is 0 but the variance may be large.
  \item How does this happen? Recall that the least squares estimation $\hat{\beta} = (X^T X)^{-1}X^T Y$.
        \item If $(X^T X)$ is near-singular, then small changes in the X might lead to large changes in $\hat{\beta}$.
        \item So, even if our $\hat{\beta}$ fits one sample well, there is no guarantee it will fit other samples well, let alone the population! 
\end{itemize}
}
        
        
\frame{
        
\begin{itemize}        
\item Interpretation
\item A large number of independent variables can make the model difficult to interpret, especially when we want to isolate the "most important" variables.
\item Do we care about variables with very small coefficients?
\end{itemize}
}


\subsection{Subset Selection \& Ridge Regression}
\frame{
\frametitle{Improving OLS: Subset Selection}
Subset Selection
    \begin{itemize}
\item Simply ignore one or more of the independent variables! That is, set the coefficient(s) to 0.
\item This helps with interpretability, if only because there is less to interpret.
\item Drawback: Subset Selection is a discrete process. Regressors are either kept or dropped; there is no in-between. Small changes in the sampling data can thus result in very different models.
\item Not computationally practical for high dimensional data
\end{itemize}
}

\frame{
\frametitle{Improving OLS: Ridge Regression}
Ridge Regression
\begin{itemize}
\item In Ridge regression we again minimize $E(\beta) = \norm{Y-X\beta}_2^2$
\item But subject to the constraint that $$\sum_{i=1}^{p-1}\beta_i^2 = \norm{\beta}_2^2 \leq t \ \text{for} \ t\geq 0$$
\end{itemize}

}


\frame{
\frametitle{Improving OLS: Ridge Regression}
Ridge Regression
\begin{itemize}
\item This is equivalent to the unconstrained minimization of
\[
E^R(\beta)=\norm{Y=X\beta}_2^2 + \lambda \norm{\beta}_2^2
\]
where $\lambda$ is a function of $t$
\end{itemize}

}


\frame{
\frametitle{Improving OLS: Ridge Regression}
Ridge Regression
\begin{itemize}
\item In terms of the coefficients, $\hat{\beta}^{R}$, it is equivalent to adding a small constant value $\lambda$ to diagonal entries of $(X^T X)$ in the OLS solution
\item This prevents the matrix from being singular or near-singular.
\item Drawback: Reduces the variance, but does not set any coefficients to 0, so it doesn't help with interpretability. It also adds bias.
\end{itemize}

}

\section{Lasso}

\frame{
\begin{multicols}{2}
 \tableofcontents[currentsection, hideothersubsections]
\end{multicols}
}


\frame{
    \frametitle{Enter the Lasso}
    \begin{itemize}
        \item L.A.S.S.O: Least Absolute Shrinkage and Selection Operator.
        \item Like Ridge we minimize $E(\beta)$ under a constraint, the lasso coefficients are found by minimizing
        \[
        E^L(\beta) = \norm{Y-X\beta}_2^2 \text{ subject to } \norm{\hat{\beta}}_1^2 \leq t
        \]
        \item We are using the 1 norm, instead of the 2 norm used in ridge
        \item As in Ridge this can be written as an unconstrained optimization problem with $\lambda (t)$
    \end{itemize}

}


\subsection{Variable Selection}
\frame{
\frametitle{Variable Selection}
\begin{itemize}
\item Lasso has the desirable property that it will usually set some of the coefficients equal to zero 
\item Like in subset selection this leads to a more interpretable model overall
\item In Ridge and OLS the minimizing coefficients are almost certainly non-zero
\end{itemize}
}

\frame{
\frametitle{Why is this?}
Consider the constrained optimization problems for lasso and ridge in the case of two variables
$$picture of constrained problem$$
}

\subsection{Properties of the Estimates}
\frame{
\begin{itemize}
\item As in Ridge and subset selection the Lasso enjoys lower variance in the magnitude of coefficients than OLS
\item Why? I'm not sure...

\end{itemize}

}

\addtocontents{toc}{\newpage}
\section[]{Tuning Parameter}


\frame{
\begin{multicols}{2}
 \tableofcontents[currentsection, hideothersubsections]
\end{multicols}
}


\frame{
    \frametitle{Tuning parameter}
    \begin{itemize}
        \item t or $\lambda(t)$ determines the amount of shrinkage we apply to the OLS estimates
        \item Let $\hat{\beta}^o = 
        \begin{bmatrix}
           \hat{\beta}_{1} \\
           \hat{\beta}_{2} \\
           \vdots \\
           \hat{\beta}_{p}
         \end{bmatrix}$ 
         be the OLS coefficients
         \item Define $t_{o} = \sum_{j=1}^p |B_{j}|$, at $t \geq t_o$ we recover the OLS estimates
    \end{itemize}
}

\frame{
\frametitle{Tuning parameter}

    In order to obtain an optimal estimate, of the $\hat{\beta}$ coefficients, the
    tuning parameter $t$ must be chosen carefully. Recall that we saw the tuning parameter as a 
             \item Setting $t \leq t_{0}$ will \color{red} shrink \color{black} the solutions toward 0.
             \item For example, setting $t = \frac{t_{0}}{2}$ is similar to selecting the best subset containing half of the regressors. 


}


\frame{
\frametitle{Why is this?}
In the 2-D case this looks like
$$picture of constrained problem$$
}


\frame{
\frametitle{Selecting t or $\lambda(t)$}
\begin{itemize}
\item
In the equivalent unconstrained problem $\lambda(t)$ is a function of t
\[
E^L(\beta)=\norm{Y=X\beta}_2^2 + \lambda \norm{\beta}_1^2
\]
\item
Because the 
\end{itemize}
}

\subsection{Cross-Validation}
\subsection{BIC}
\subsection{Shooting Method}

\section{Examples}


\frame{
\begin{multicols}{2}
 \tableofcontents[currentsection, hideothersubsections]
\end{multicols}
}


\subsection{Prostate Cancer}
\subsection{Oil and Gas}
\subsection{House Prices}





%Examples

%Simulations
\section{Simulation Analysis}

\frame{
\begin{multicols}{2}
 \tableofcontents[currentsection, hideothersubsections]
\end{multicols}
}



%Adaptive Lasso
\section{Adaptive Lasso}

\frame{
\begin{multicols}{2}
 \tableofcontents[currentsection, hideothersubsections]
\end{multicols}
}

\frame{
    
    \begin{itemize}
        \item {\color{purple}$y_1= \beta_0 + \beta_1 x_1 + \beta_2 x_2+ \dots +\beta_{p-1} x_{p-1}$}
        \item
        $\hat{\beta}(lasso) = argmin{\underset{\beta}} \norm{y- \sum_{j-1}^p x_j \beta_j}^2 + \lambda \sum_{j=1}^p |\beta|$
    \end{itemize}


}


\subsection{Oracle Property}
%Oracle Property










\section[Conclusion]{}

\frame{
    \frametitle{Lasso vs. Subset Selection vs. Ridge Regression}
    \begin{itemize}
        \item Small number of large effects.
        \begin{itemize}
            \item Subset selection is best, Lasso is second best, Ridge Regression is worst.
        \end{itemize}
        \item Small to moderate number of moderate-sized effects.
        \begin{itemize}
            \item Lasso is best, Ridge regression is second best, Subset selection is worst.
        \end{itemize}
        \item Large of small effects.
        \begin{itemize}
            \item Ridge regression is best, Lasso is second best, Subset selection is worst.
        \end{itemize}
    \end{itemize}
    
  
   
}


\frame{
    \frametitle{Lasso vs. Subset Selection vs. Ridge Regression}
    
    \begin{itemize}
        \item Do these results make sense?
        \begin{itemize}
            \item Recall that the Lasso was designed to work like Ridge regression but with some of the benefits of Subset selection.
            \item It thus makes sense for the Lasso to fall between Ridge regression and Subset selection on extreme cases and to beat both of them on cases not well-suited to either.
        \end{itemize}
        \item {\color{red} CAUTION!}
        \begin{itemize}
            \item These results refer to \color{red} prediction accuracy \color{black}
            \item As for {\color{blue} interpretability:}
            \begin{itemize}
                \item Subset selection $>$ Lasso $>$ Ridge regression, always
                \item Why? More nonzero coefficients = more interpretable, always
            \end{itemize}
        \end{itemize}
    \end{itemize}
}


\frame{
\frametitle{References}



}



\end{document}

